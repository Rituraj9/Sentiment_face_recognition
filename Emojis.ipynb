{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'train'\n",
    "val_dir = 'test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 1395s 3s/step - loss: 1.8075 - accuracy: 0.2563 - val_loss: 1.6891 - val_accuracy: 0.3164\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 235s 524ms/step - loss: 1.6475 - accuracy: 0.3566 - val_loss: 1.4876 - val_accuracy: 0.4082\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 1.5397 - accuracy: 0.4082 - val_loss: 1.6616 - val_accuracy: 0.4334\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 234s 523ms/step - loss: 1.4626 - accuracy: 0.4400 - val_loss: 1.3673 - val_accuracy: 0.4651\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 238s 530ms/step - loss: 1.4002 - accuracy: 0.4637 - val_loss: 1.4135 - val_accuracy: 0.4800\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 238s 532ms/step - loss: 1.3482 - accuracy: 0.4877 - val_loss: 1.2609 - val_accuracy: 0.4983\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 287s 641ms/step - loss: 1.3008 - accuracy: 0.5079 - val_loss: 1.3463 - val_accuracy: 0.5112\n",
      "Epoch 8/50\n",
      "444/448 [============================>.] - ETA: 1:40 - loss: 1.2657 - accuracy: 0.5198"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:616: UserWarning: The input 415 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 33950s 76s/step - loss: 1.2655 - accuracy: 0.5200 - val_loss: 1.4125 - val_accuracy: 0.5197\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 1101s 2s/step - loss: 1.2341 - accuracy: 0.5364 - val_loss: 1.2443 - val_accuracy: 0.5388\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 227s 507ms/step - loss: 1.2047 - accuracy: 0.5471 - val_loss: 1.2528 - val_accuracy: 0.5335\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 295s 659ms/step - loss: 1.1786 - accuracy: 0.5583 - val_loss: 1.1040 - val_accuracy: 0.5500\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 325s 726ms/step - loss: 1.1527 - accuracy: 0.5677 - val_loss: 1.1604 - val_accuracy: 0.5450\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 254s 567ms/step - loss: 1.1217 - accuracy: 0.5832 - val_loss: 1.1600 - val_accuracy: 0.5648\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 227s 507ms/step - loss: 1.1019 - accuracy: 0.5882 - val_loss: 1.3819 - val_accuracy: 0.5638\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 226s 505ms/step - loss: 1.0754 - accuracy: 0.5983 - val_loss: 1.2766 - val_accuracy: 0.5765\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 227s 507ms/step - loss: 1.0520 - accuracy: 0.6080 - val_loss: 1.0541 - val_accuracy: 0.5745\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 227s 508ms/step - loss: 1.0277 - accuracy: 0.6169 - val_loss: 1.0637 - val_accuracy: 0.5728\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 237s 529ms/step - loss: 1.0125 - accuracy: 0.6261 - val_loss: 1.1713 - val_accuracy: 0.5835\n",
      "Epoch 19/50\n",
      " 19/448 [>.............................] - ETA: 32:23 - loss: 0.9766 - accuracy: 0.6283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:616: UserWarning: The input 148 could not be retrieved. It could be because a worker has died.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 5655s 13s/step - loss: 0.9842 - accuracy: 0.6374 - val_loss: 1.0485 - val_accuracy: 0.5971\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 243s 543ms/step - loss: 0.9601 - accuracy: 0.6422 - val_loss: 0.9794 - val_accuracy: 0.5984\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 229s 510ms/step - loss: 0.9429 - accuracy: 0.6530 - val_loss: 0.9586 - val_accuracy: 0.5915\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 236s 528ms/step - loss: 0.9219 - accuracy: 0.6573 - val_loss: 0.8689 - val_accuracy: 0.6011\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 238s 530ms/step - loss: 0.8961 - accuracy: 0.6709 - val_loss: 0.9528 - val_accuracy: 0.5971\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 237s 529ms/step - loss: 0.8802 - accuracy: 0.6761 - val_loss: 0.9209 - val_accuracy: 0.6046\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 238s 530ms/step - loss: 0.8538 - accuracy: 0.6828 - val_loss: 1.0848 - val_accuracy: 0.6028\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 237s 529ms/step - loss: 0.8326 - accuracy: 0.6927 - val_loss: 1.1885 - val_accuracy: 0.6146\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 228s 510ms/step - loss: 0.8035 - accuracy: 0.7067 - val_loss: 1.1313 - val_accuracy: 0.5971\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 228s 508ms/step - loss: 0.7979 - accuracy: 0.7088 - val_loss: 0.9682 - val_accuracy: 0.6147\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 228s 509ms/step - loss: 0.7736 - accuracy: 0.7167 - val_loss: 1.1973 - val_accuracy: 0.6140\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 227s 507ms/step - loss: 0.7483 - accuracy: 0.7259 - val_loss: 0.7428 - val_accuracy: 0.6056\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 232s 517ms/step - loss: 0.7297 - accuracy: 0.7336 - val_loss: 1.1307 - val_accuracy: 0.6189\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 241s 537ms/step - loss: 0.7091 - accuracy: 0.7416 - val_loss: 0.8268 - val_accuracy: 0.6172\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 249s 556ms/step - loss: 0.6827 - accuracy: 0.7519 - val_loss: 0.7261 - val_accuracy: 0.6144\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 271s 605ms/step - loss: 0.6658 - accuracy: 0.7584 - val_loss: 0.9743 - val_accuracy: 0.6313\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 271s 605ms/step - loss: 0.6394 - accuracy: 0.7680 - val_loss: 1.3898 - val_accuracy: 0.6241\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 254s 567ms/step - loss: 0.6256 - accuracy: 0.7729 - val_loss: 1.2200 - val_accuracy: 0.6192\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 253s 564ms/step - loss: 0.6049 - accuracy: 0.7800 - val_loss: 1.0754 - val_accuracy: 0.6186\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 270s 604ms/step - loss: 0.5881 - accuracy: 0.7876 - val_loss: 0.8550 - val_accuracy: 0.6257\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 271s 604ms/step - loss: 0.5706 - accuracy: 0.7919 - val_loss: 1.2256 - val_accuracy: 0.6222\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 271s 605ms/step - loss: 0.5486 - accuracy: 0.8003 - val_loss: 0.8254 - val_accuracy: 0.6240\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 264s 589ms/step - loss: 0.5308 - accuracy: 0.8074 - val_loss: 1.0964 - val_accuracy: 0.6272\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 236s 527ms/step - loss: 0.5216 - accuracy: 0.8109 - val_loss: 0.9365 - val_accuracy: 0.6264\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 228s 508ms/step - loss: 0.4981 - accuracy: 0.8212 - val_loss: 1.0702 - val_accuracy: 0.6326\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 233s 521ms/step - loss: 0.4804 - accuracy: 0.8292 - val_loss: 1.2905 - val_accuracy: 0.6231\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 238s 531ms/step - loss: 0.4763 - accuracy: 0.8262 - val_loss: 1.1373 - val_accuracy: 0.6320\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 242s 541ms/step - loss: 0.4590 - accuracy: 0.8356 - val_loss: 1.0770 - val_accuracy: 0.6295\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 230s 513ms/step - loss: 0.4432 - accuracy: 0.8391 - val_loss: 1.0857 - val_accuracy: 0.6296\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 239s 534ms/step - loss: 0.4353 - accuracy: 0.8426 - val_loss: 1.0365 - val_accuracy: 0.6238\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 253s 565ms/step - loss: 0.4127 - accuracy: 0.8517 - val_loss: 1.1398 - val_accuracy: 0.6257\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 258s 576ms/step - loss: 0.4025 - accuracy: 0.8550 - val_loss: 1.0742 - val_accuracy: 0.6297\n"
     ]
    }
   ],
   "source": [
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        #print(emotion_prediction)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Rituraj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "\n",
    "\n",
    "emoji_dist={0:\"./emojis/angry.png\",2:\"./emojis/disgusted.png\",2:\"./emojis/fearful.png\",3:\"./emojis/happy1.png\",4:\"./emojis/neutral.png\",5:\"./emojis/sad.png\",6:\"./emojis/surpriced.png\"}\n",
    "\n",
    "global last_frame1                                    \n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "\n",
    "def show_vid():      \n",
    "    cap1 = cv2.VideoCapture(0)                                 \n",
    "    if not cap1.isOpened():                             \n",
    "        print(\"cant open the camera1\")\n",
    "    flag1, frame1 = cap1.read()\n",
    "    #cv2.imshow(\"Image\",frame1)\n",
    "    #print(frame1)\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "    \n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"Image\",gray_frame)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "        #print(show_text)\n",
    "        \n",
    "    if flag1 is None:\n",
    "        print (\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        #print(last_frame1)\n",
    "        pic = cv2.cvtColor(last_frame1,cv2.COLOR_BGR2RGB)\n",
    "        #print(pic)\n",
    "        img = Image.fromarray(pic)\n",
    "        #print(img)\n",
    "        #PhotoImage(master=root)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        #print(imgtk)\n",
    "        lmain.imgtk = imgtk\n",
    "        #print(lmain.imgtk)\n",
    "        lmain.configure(image=imgtk)\n",
    "        #print(\"hi\")\n",
    "        lmain.after(10, show_vid)\n",
    "        \n",
    "    #cv2.imshow('frame',cap1)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "\n",
    "\n",
    "def show_vid2():\n",
    "    frame2=cv2.imread(emoji_dist[show_text[0]])\n",
    "    #print(frame2)\n",
    "    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "    img2=Image.fromarray(pic2)\n",
    "    imgtk2=ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2=imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))\n",
    "    \n",
    "    lmain2.configure(image=imgtk2)\n",
    "    lmain2.after(10, show_vid2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root=Tk()\n",
    "    #print(\"hi\")\n",
    "    img = ImageTk.PhotoImage(Image.open(\"logo.png\"))\n",
    "    #print(\"hi\")\n",
    "    heading = Label(image=img,bg='black')\n",
    "    #print(\"hi\")\n",
    "    heading.pack() \n",
    "    heading2=Label(root,text=\"Photo to Emoji\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')                                 \n",
    "    \n",
    "    heading2.pack()\n",
    "    #print(\"hi\")\n",
    "    \n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    #lmain.grid(row=0, column=0)\n",
    "    \n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    \n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    \n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    \n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    \n",
    "    #print(\"hi\")\n",
    "    root.title(\"Photo To Emoji\")            \n",
    "    root.geometry(\"1400x900+100+10\") \n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    show_vid()\n",
    "    show_vid2()\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
